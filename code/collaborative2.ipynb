{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_distances\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# import and run file\n",
    "from importnb import imports\n",
    "with imports(\"ipynb\"):\n",
    "    import data_preparation\n",
    "%run data_preparation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate data for each unique user\n",
    "data = reviews[['Reviewer Name', 'Birth Year', 'Marital Status', 'Has Children?',\n",
    "                'Vegetarian?', 'Weight (lb)', 'Height (in)', 'Average Amount Spent',\n",
    "                'Preferred Mode of Transport', 'Northwestern Student?']]\n",
    "\n",
    "data_users = data.groupby(['Reviewer Name']).agg(**{\n",
    "    'Birth Year': ('Birth Year', lambda x: pd.Series.mode(x)[0]),\n",
    "    'Marital Status': ('Marital Status', lambda x: pd.Series.mode(x)[0]),\n",
    "    'Has Children?': ('Has Children?', lambda x: pd.Series.mode(x)[0]),\n",
    "    'Vegetarian?': ('Vegetarian?', lambda x: pd.Series.mode(x)[0]),\n",
    "    'Weight (lb)': ('Weight (lb)', 'mean'),\n",
    "    'Height (in)': ('Height (in)', 'mean'),\n",
    "    'Average Amount Spent': ('Average Amount Spent', lambda x: pd.Series.mode(x)[0]),\n",
    "    'Preferred Mode of Transport': ('Preferred Mode of Transport', lambda x: pd.Series.mode(x)[0]),\n",
    "    'Northwestern Student?': ('Northwestern Student?', lambda x: pd.Series.mode(x)[0])\n",
    "}).reset_index()\n",
    "\n",
    "data_users = pd.get_dummies(\n",
    "    data_users,\n",
    "    columns=['Marital Status', 'Has Children?', 'Vegetarian?', 'Average Amount Spent',\n",
    "             'Preferred Mode of Transport', 'Northwestern Student?'],\n",
    "    drop_first=True, dtype=int\n",
    ")\n",
    "\n",
    "data_demographics = data_users.drop(columns=['Reviewer Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user scores of each restaurant\n",
    "data_scores = reviews[['Reviewer Name', 'Restaurant Name', 'Rating']].groupby(['Reviewer Name', 'Restaurant Name']).agg(Rating = ('Rating', 'mean')).reset_index()\n",
    "data_scores_table = data_scores.pivot(index='Restaurant Name', columns='Reviewer Name', values='Rating').reset_index()\n",
    "# only consider restaurants that we have information on\n",
    "restaurants_considered = list(restaurants['Restaurant Name'])\n",
    "restaurants_considered.remove('Evanston Games & Cafe')\n",
    "data_scores_table = data_scores_table[data_scores_table['Restaurant Name'].isin(restaurants_considered)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tokyo\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1446: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# k-means clustering in order to impute sparseness\n",
    "\n",
    "# first prepare the data for clustering\n",
    "data_users_num = data_demographics[['Birth Year', 'Weight (lb)', 'Height (in)']]\n",
    "data_users_cat = data_demographics.drop(columns=['Birth Year', 'Weight (lb)', 'Height (in)'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_users_scaled = scaler.fit_transform(data_users_num)\n",
    "data_users_scaled = np.column_stack((data_users_scaled, data_users_cat))\n",
    "\n",
    "# find which cluster than each user belong to\n",
    "kmeans_labels = KMeans(n_clusters=4, n_init=10, max_iter=10).fit_predict(data_users_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge user names with which cluster they belong to\n",
    "data_users_clustered = np.column_stack((data_users['Reviewer Name'], kmeans_labels))\n",
    "data_users_clustered = pd.DataFrame(data_users_clustered, columns=['Reviewer Name', 'cluster'])\n",
    "data_scores_clustered = data_scores.merge(data_users_clustered, left_on='Reviewer Name', right_on='Reviewer Name')\n",
    "# find average rating by restaurant for each cluster\n",
    "avg_scores_clustered = data_scores_clustered.groupby(['cluster', 'Restaurant Name'])['Rating'].mean().unstack(level=0)\n",
    "\n",
    "# if ratings still missing, impute restaurant rating as average rating of each cluster\n",
    "avg_scores_clustered = avg_scores_clustered.apply(lambda row: row.fillna(row.mean()), axis=1).reset_index()\n",
    "avg_scores_clustered = pd.melt(avg_scores_clustered, id_vars='Restaurant Name', value_vars=[0, 1, 2, 3]).reset_index()\n",
    "# merge user name with their cluster data\n",
    "data_user_avg_scores_clustered = data_users_clustered.merge(avg_scores_clustered, how='right', on='cluster')\n",
    "data_user_avg_scores_clustered = data_user_avg_scores_clustered[['Reviewer Name', 'Restaurant Name', 'value']]\n",
    "data_avg_scores_table = data_user_avg_scores_clustered.pivot(index='Restaurant Name', columns='Reviewer Name', values='value').reset_index()\n",
    "data_avg_scores_table = data_avg_scores_table[data_avg_scores_table['Restaurant Name'].isin(restaurants_considered)]\n",
    "# impute missingness in user scores of each restaurant\n",
    "data_scores_table_complete = data_scores_table.fillna(data_avg_scores_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_scores_cosine = pd.DataFrame(\n",
    "    cosine_distances(\n",
    "        data_scores_table_complete.drop(columns=['Restaurant Name']).T,\n",
    "        data_scores_table_complete.drop(columns=['Restaurant Name']).T\n",
    "    ),\n",
    "    columns=data_scores_table_complete.columns[1:],\n",
    "    index=data_scores_table_complete.columns[1:]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collab_filter_scores(name, n_similar):\n",
    "    most_similar_users = user_scores_cosine[user_scores_cosine.index != name][name].sort_values(ascending=True).index[0:n_similar]\n",
    "    possible_recs = reviews[reviews['Reviewer Name'].isin(most_similar_users)].groupby(['Reviewer Name'])\n",
    "    top_recs = possible_recs.apply(lambda x: x[x['Rating'] == x['Rating'].max()])\n",
    "    return( top_recs[['Restaurant Name', 'Rating']].reset_index().drop(columns=['level_1']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(collab_filter_scores('Timothy Mace', 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
